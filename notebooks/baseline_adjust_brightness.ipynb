{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    " ## Cloud Cover Semantic Segmentation\n",
    "### Final project: Baseline\n",
    "\n",
    "Project Team: <br>\n",
    "Kurt Eulau <br>\n",
    "Steve Hewitt <br>\n",
    "Tom Welsh <br>\n",
    "\n",
    "\n",
    "Citations: <br>\n",
    " - TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives: \n",
    " - TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1: Import packages\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas_path pytorch_lightning cloudpathlib loguru typer\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_path import path  # noqa\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import rasterio\n",
    "import pyproj\n",
    "import rioxarray\n",
    "import xrspatial.multispectral as ms\n",
    "from my_preprocessing import train_val_test_split\n",
    "import segmentation_models_pytorch as smp\n",
    "from typing import Optional, List\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1A (Optional): Log training to wandb.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_lightning.loggers import WandbLogger\n",
    "# from pytorch_lightning import Trainer\n",
    "\n",
    "# wandb_logger = WandbLogger(project=\"baseline-adjust-brightness-v1\", entity=\"w207-clouds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: Define working directories and global variables\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/driven-data/cloud-cover\")\n",
    "TRAIN_FEATURES = DATA_DIR / \"train_features\"\n",
    "TRAIN_LABELS = DATA_DIR / \"train_labels\"\n",
    "\n",
    "assert TRAIN_FEATURES.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDS = [\"B02\", \"B03\", \"B04\", \"B08\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chip_id</th>\n",
       "      <th>location</th>\n",
       "      <th>datetime</th>\n",
       "      <th>cloudpath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adwp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adwu</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adwz</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adxp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adxp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aeaj</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/aeaj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chip_id  location              datetime                   cloudpath\n",
       "0    adwp  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwp\n",
       "1    adwu  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwu\n",
       "2    adwz  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwz\n",
       "3    adxp  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adxp\n",
       "4    aeaj  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/aeaj"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta = pd.read_csv(DATA_DIR / \"train_metadata.csv\")\n",
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create benchmark_src folder\n",
    "submission_dir = Path(\"benchmark_src\")\n",
    "if submission_dir.exists():\n",
    "    shutil.rmtree(submission_dir)\n",
    "\n",
    "submission_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Create scripts to define loss function and classes.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing benchmark_src/losses.py\n"
     ]
    }
   ],
   "source": [
    "%%file {submission_dir}/losses.py\n",
    "import numpy as np\n",
    "\n",
    "def intersection_over_union(pred, true):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates intersection and union for a batch of images.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): a tensor of predictions\n",
    "        true (torc.Tensor): a tensor of labels\n",
    "\n",
    "    Returns:\n",
    "        intersection (int): total intersection of pixels\n",
    "        union (int): total union of pixels\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    valid_pixel_mask = true.ne(255)  # valid pixel mask\n",
    "    true = true.masked_select(valid_pixel_mask).to(\"cpu\")\n",
    "    pred = pred.masked_select(valid_pixel_mask).to(\"cpu\")\n",
    "\n",
    "    # Intersection and union totals\n",
    "    intersection = np.logical_and(true, pred)\n",
    "    union = np.logical_or(true, pred)\n",
    "    return intersection.sum() / union.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing benchmark_src/cloud_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%file {submission_dir}/cloud_dataset.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import torch\n",
    "from typing import Optional, List\n",
    "\n",
    "class CloudDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Reads in images, transforms pixel values, and serves a\n",
    "    dictionary containing chip ids, image tensors, and\n",
    "    label masks (where available).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_paths: pd.DataFrame, # : syntax specifies datatype for each parameter for CloudDataset objects\n",
    "        bands: List[str],\n",
    "        y_paths: Optional[pd.DataFrame] = None,\n",
    "        transforms: Optional[list] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiate the CloudDataset class.\n",
    "\n",
    "        Args:\n",
    "            x_paths (pd.DataFrame): a dataframe with a row for each chip. There must be a column for chip_id,\n",
    "                and a column with the path to the TIF for each of bands\n",
    "            bands (list[str]): list of the bands included in the data\n",
    "            y_paths (pd.DataFrame, optional): a dataframe with a for each chip and columns for chip_id\n",
    "                and the path to the label TIF with ground truth cloud cover\n",
    "            transforms (list, optional): list of transforms to apply to the feature data (eg augmentations)\n",
    "        \"\"\"\n",
    "        self.data = x_paths\n",
    "        self.label = y_paths\n",
    "        self.transforms = transforms\n",
    "        self.bands = bands\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # Similar to pop, helps iterate through dataset\n",
    "    def __getitem__(self, idx: int):\n",
    "        # Loads an n-channel image from a chip-level dataframe\n",
    "        img = self.data.loc[idx]\n",
    "        band_arrs = []\n",
    "        for band in self.bands:\n",
    "            with rasterio.open(img[f\"{band}_path\"]) as b:\n",
    "                band_arr = b.read(1).astype(\"float32\")\n",
    "            band_arrs.append(band_arr)\n",
    "        x_arr = np.stack(band_arrs, axis=-1) # 3-dimensional array\n",
    "\n",
    "        # Apply data augmentations, if provided\n",
    "        if self.transforms:\n",
    "            x_arr = self.transforms(image=x_arr)[\"image\"]\n",
    "        x_arr = np.transpose(x_arr, [2, 0, 1]) # re-orders array to match expected format needed for model\n",
    "        \n",
    "        # Prepare dictionary for item\n",
    "        item = {\"chip_id\": img.chip_id, \"chip\": x_arr}\n",
    "\n",
    "        # Load label if available\n",
    "        if self.label is not None:\n",
    "            label_path = self.label.loc[idx].label_path\n",
    "            with rasterio.open(label_path) as lp:\n",
    "                y_arr = lp.read(1).astype(\"float32\")\n",
    "            # Apply same data augmentations to the label\n",
    "            if self.transforms:\n",
    "                y_arr = self.transforms(image=y_arr)[\"image\"]\n",
    "            item[\"label\"] = y_arr\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color='red'>Add transformation from albumentations<font></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing benchmark_src/cloud_model.py\n"
     ]
    }
   ],
   "source": [
    "%%file {submission_dir}/cloud_model.py\n",
    "from typing import Optional, List\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import albumentations as A # ADDED\n",
    "import torchvision # ADDED\n",
    "\n",
    "from benchmark_src.cloud_dataset import CloudDataset\n",
    "\n",
    "try:\n",
    "    from cloud_dataset import CloudDataset\n",
    "    from losses import intersection_over_union\n",
    "except ImportError:\n",
    "    from benchmark_src.cloud_dataset import CloudDataset\n",
    "    from benchmark_src.losses import intersection_over_union\n",
    "\n",
    "class CloudModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bands: List[str],\n",
    "        x_train: Optional[pd.DataFrame] = None,\n",
    "        y_train: Optional[pd.DataFrame] = None,\n",
    "        x_val: Optional[pd.DataFrame] = None,\n",
    "        y_val: Optional[pd.DataFrame] = None,\n",
    "        hparams: dict = {},\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiate the CloudModel class based on the pl.LightningModule\n",
    "        (https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html).\n",
    "\n",
    "        Args:\n",
    "            bands (list[str]): Names of the bands provided for each chip\n",
    "            x_train (pd.DataFrame, optional): a dataframe of the training features with a row for each chip.\n",
    "                There must be a column for chip_id, and a column with the path to the TIF for each of bands.\n",
    "                Required for model training\n",
    "            y_train (pd.DataFrame, optional): a dataframe of the training labels with a for each chip\n",
    "                and columns for chip_id and the path to the label TIF with ground truth cloud cover.\n",
    "                Required for model training\n",
    "            x_val (pd.DataFrame, optional): a dataframe of the validation features with a row for each chip.\n",
    "                There must be a column for chip_id, and a column with the path to the TIF for each of bands.\n",
    "                Required for model training\n",
    "            y_val (pd.DataFrame, optional): a dataframe of the validation labels with a for each chip\n",
    "                and columns for chip_id and the path to the label TIF with ground truth cloud cover.\n",
    "                Required for model training\n",
    "            hparams (dict, optional): Dictionary of additional modeling parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hparams.update(hparams)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # required\n",
    "        self.bands = bands\n",
    "\n",
    "        # optional modeling params\n",
    "        self.backbone = self.hparams.get(\"backbone\", \"resnet34\")\n",
    "        self.weights = self.hparams.get(\"weights\", \"imagenet\")\n",
    "        self.learning_rate = self.hparams.get(\"lr\", 1e-3)\n",
    "        self.patience = self.hparams.get(\"patience\", 4)\n",
    "        self.num_workers = self.hparams.get(\"num_workers\", 2)\n",
    "        self.batch_size = self.hparams.get(\"batch_size\", 16)\n",
    "        self.gpu = self.hparams.get(\"gpu\", False)\n",
    "        \n",
    "        # ADDED BRIGHTNESS/CONTRAST TRANSFORMATION WITH DEFAULT COLORJITTER PARAMS\n",
    "        self.transform = A.Compose([A.ColorJitter()])\n",
    "        #self.transform = torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)\n",
    "        \n",
    "        # Instantiate datasets, model, and trainer params if provided\n",
    "        self.train_dataset = CloudDataset(\n",
    "            x_paths=x_train,\n",
    "            bands=self.bands,\n",
    "            y_paths=y_train,\n",
    "            transforms=self.transform,\n",
    "        )\n",
    "        self.val_dataset = CloudDataset(\n",
    "            x_paths=x_val,\n",
    "            bands=self.bands,\n",
    "            y_paths=y_val,\n",
    "            transforms=None,\n",
    "        )\n",
    "        self.model = self._prepare_model()\n",
    "\n",
    "    ## Required LightningModule methods ##\n",
    "\n",
    "    def forward(self, image: torch.Tensor):\n",
    "        # Forward pass\n",
    "        return self.model(image)\n",
    "\n",
    "    def training_step(self, batch: dict, batch_idx: int):\n",
    "        \"\"\"\n",
    "        Training step.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): dictionary of items from CloudDataset of the form\n",
    "                {'chip_id': list[str], 'chip': list[torch.Tensor], 'label': list[torch.Tensor]}\n",
    "            batch_idx (int): batch number\n",
    "        \"\"\"\n",
    "        if self.train_dataset.data is None:\n",
    "            raise ValueError(\n",
    "                \"x_train and y_train must be specified when CloudModel is instantiated to run training\"\n",
    "            )\n",
    "\n",
    "        # Switch on training mode\n",
    "        self.model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        y = batch[\"label\"].long()\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "        # Forward pass\n",
    "        preds = self.forward(x)\n",
    "\n",
    "        # Log batch loss\n",
    "        loss = torch.nn.CrossEntropyLoss(reduction=\"none\")(preds, y).mean()\n",
    "        self.log(\n",
    "            \"loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: dict, batch_idx: int):\n",
    "        \"\"\"\n",
    "        Validation step.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): dictionary of items from CloudDataset of the form\n",
    "                {'chip_id': list[str], 'chip': list[torch.Tensor], 'label': list[torch.Tensor]}\n",
    "            batch_idx (int): batch number\n",
    "        \"\"\"\n",
    "        if self.val_dataset.data is None:\n",
    "            raise ValueError(\n",
    "                \"x_val and y_val must be specified when CloudModel is instantiated to run validation\"\n",
    "            )\n",
    "\n",
    "        # Switch on validation mode\n",
    "        self.model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        y = batch[\"label\"].long()\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "        # Forward pass & softmax\n",
    "        preds = self.forward(x)\n",
    "        preds = torch.softmax(preds, dim=1)[:, 1]\n",
    "        preds = (preds > 0.5) * 1  # convert to int\n",
    "\n",
    "        # Log batch IOU\n",
    "        batch_iou = intersection_over_union(preds, y)\n",
    "        self.log(\n",
    "            \"iou\", batch_iou, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return batch_iou\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # DataLoader class for training\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # DataLoader class for validation\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=0,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\n",
    "        return [opt], [sch]\n",
    "\n",
    "    ## Convenience Methods ##\n",
    "\n",
    "    def _prepare_model(self):\n",
    "        # Instantiate U-Net model\n",
    "        unet_model = smp.Unet(\n",
    "            encoder_name=self.backbone,\n",
    "            encoder_weights=self.weights,\n",
    "            in_channels=4,\n",
    "            classes=2,\n",
    "        )\n",
    "        if self.gpu:\n",
    "            unet_model.cuda()\n",
    "\n",
    "        return unet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the cloud model we just saved.\n",
    "from benchmark_src.cloud_model import CloudModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 4: Define functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_paths(df, feature_dir, label_dir=None, bands=BANDS):\n",
    "    \"\"\"\n",
    "    Given dataframe with a column for chip_id, returns a dataframe with a column\n",
    "    added indicating the path to each band's TIF image as \"{band}_path\", eg \"B02_path\".\n",
    "    A column is also added to the dataframe with paths to the label TIF, if the\n",
    "    path to the labels directory is provided.\n",
    "    \"\"\"\n",
    "    for band in bands:\n",
    "        df[f\"{band}_path\"] = feature_dir / df[\"chip_id\"] / f\"{band}.tif\"\n",
    "        # make sure a random sample of paths exist\n",
    "        assert df.sample(n=40, random_state=5)[f\"{band}_path\"].path.exists().all()\n",
    "    if label_dir is not None:\n",
    "        df[\"label_path\"] = label_dir / (df[\"chip_id\"] + \".tif\")\n",
    "        # make sure a random sample of paths exist\n",
    "        assert df.sample(n=40, random_state=5)[\"label_path\"].path.exists().all()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 5: Read data and preprocess\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chip_id</th>\n",
       "      <th>location</th>\n",
       "      <th>datetime</th>\n",
       "      <th>cloudpath</th>\n",
       "      <th>B02_path</th>\n",
       "      <th>B03_path</th>\n",
       "      <th>B04_path</th>\n",
       "      <th>B08_path</th>\n",
       "      <th>label_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adwp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwp</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/adwp.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adwu</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwu</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwu/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwu/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwu/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwu/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/adwu.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adwz</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwz</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwz/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwz/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwz/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwz/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/adwz.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adxp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adxp</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adxp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adxp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adxp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adxp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/adxp.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aeaj</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/aeaj</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeaj/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeaj/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeaj/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeaj/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/aeaj.tif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chip_id  location              datetime                   cloudpath  \\\n",
       "0    adwp  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwp   \n",
       "1    adwu  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwu   \n",
       "2    adwz  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwz   \n",
       "3    adxp  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adxp   \n",
       "4    aeaj  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/aeaj   \n",
       "\n",
       "                                            B02_path  \\\n",
       "0  /driven-data/cloud-cover/train_features/adwp/B...   \n",
       "1  /driven-data/cloud-cover/train_features/adwu/B...   \n",
       "2  /driven-data/cloud-cover/train_features/adwz/B...   \n",
       "3  /driven-data/cloud-cover/train_features/adxp/B...   \n",
       "4  /driven-data/cloud-cover/train_features/aeaj/B...   \n",
       "\n",
       "                                            B03_path  \\\n",
       "0  /driven-data/cloud-cover/train_features/adwp/B...   \n",
       "1  /driven-data/cloud-cover/train_features/adwu/B...   \n",
       "2  /driven-data/cloud-cover/train_features/adwz/B...   \n",
       "3  /driven-data/cloud-cover/train_features/adxp/B...   \n",
       "4  /driven-data/cloud-cover/train_features/aeaj/B...   \n",
       "\n",
       "                                            B04_path  \\\n",
       "0  /driven-data/cloud-cover/train_features/adwp/B...   \n",
       "1  /driven-data/cloud-cover/train_features/adwu/B...   \n",
       "2  /driven-data/cloud-cover/train_features/adwz/B...   \n",
       "3  /driven-data/cloud-cover/train_features/adxp/B...   \n",
       "4  /driven-data/cloud-cover/train_features/aeaj/B...   \n",
       "\n",
       "                                            B08_path  \\\n",
       "0  /driven-data/cloud-cover/train_features/adwp/B...   \n",
       "1  /driven-data/cloud-cover/train_features/adwu/B...   \n",
       "2  /driven-data/cloud-cover/train_features/adwz/B...   \n",
       "3  /driven-data/cloud-cover/train_features/adxp/B...   \n",
       "4  /driven-data/cloud-cover/train_features/aeaj/B...   \n",
       "\n",
       "                                       label_path  \n",
       "0  /driven-data/cloud-cover/train_labels/adwp.tif  \n",
       "1  /driven-data/cloud-cover/train_labels/adwu.tif  \n",
       "2  /driven-data/cloud-cover/train_labels/adwz.tif  \n",
       "3  /driven-data/cloud-cover/train_labels/adxp.tif  \n",
       "4  /driven-data/cloud-cover/train_labels/aeaj.tif  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta = add_paths(train_meta, TRAIN_FEATURES, TRAIN_LABELS)\n",
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = train_val_test_split(train_meta, column='location', pct_train=0.6, pct_val=0.2, pct_test=0.2, random_state=42)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7017, 9), (2349, 9), (2382, 9))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features from labels\n",
    "feature_cols = [\"chip_id\"] + [f\"{band}_path\" for band in BANDS]\n",
    "\n",
    "train_x = train[feature_cols].copy()\n",
    "train_y = train[[\"chip_id\", \"label_path\"]].copy()\n",
    "\n",
    "val_x = val[feature_cols].copy()\n",
    "val_y = val[[\"chip_id\", \"label_path\"]].copy()\n",
    "\n",
    "test_x = test[feature_cols].copy()\n",
    "test_y = test[[\"chip_id\", \"label_path\"]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 6: Model Training\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "## Set up pytorch_lightning.Trainer object\n",
    "cloud_model = CloudModel(\n",
    "    bands=BANDS,\n",
    "    x_train=train_x,\n",
    "    y_train=train_y,\n",
    "    x_val=val_x,\n",
    "    y_val=val_y,\n",
    "    hparams={\"num_workers\": 2, \"batch_size\": 8},\n",
    ")\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"iou_epoch\", mode=\"max\", verbose=True\n",
    ")\n",
    "early_stopping_callback = pl.callbacks.early_stopping.EarlyStopping(\n",
    "    monitor=\"iou_epoch\",\n",
    "    patience=(cloud_model.patience * 3),\n",
    "    mode=\"max\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    fast_dev_run=False,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "#    logger = wandb_logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | Unet | 24.4 M\n",
      "-------------------------------\n",
      "24.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.4 M    Total params\n",
      "97.759    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29db6208051459dbcb6448b9cbe1372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jovyan/ucb_mids_w207_final_project/notebooks/benchmark_src/cloud_dataset.py\", line 52, in __getitem__\n    x_arr = self.transforms(image=x_arr)[\"image\"]\n  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/albumentations/core/composition.py\", line 210, in __call__\n    data = t(force_apply=force_apply, **data)\n  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/albumentations/core/transforms_interface.py\", line 97, in __call__\n    return self.apply_with_params(params, **kwargs)\n  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/albumentations/core/transforms_interface.py\", line 112, in apply_with_params\n    res[key] = target_function(arg, **dict(params, **target_dependencies))\n  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/albumentations/augmentations/transforms.py\", line 2968, in apply\n    raise TypeError(\"ColorJitter transformation expects 1-channel or 3-channel images.\")\nTypeError: ColorJitter transformation expects 1-channel or 3-channel images.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [135]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcloud_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:740\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    735\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m     )\n\u001b[1;32m    739\u001b[0m     train_dataloaders \u001b[38;5;241m=\u001b[39m train_dataloader\n\u001b[0;32m--> 740\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:685\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;124;03mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;124;03mas all errors should funnel through them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:777\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;66;03m# TODO: ckpt_path only in v1.7\u001b[39;00m\n\u001b[1;32m    776\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m--> 777\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1199\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;66;03m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[0;32m-> 1199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;66;03m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_dispatch()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1279\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_type_plugin\u001b[38;5;241m.\u001b[39mstart_predicting(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1279\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_type_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:202\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_training\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1289\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1319\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1319\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:234\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m data_fetcher \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mget_profiled_dataloader(dataloader)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# the global step is manually decreased here due to backwards compatibility with existing loggers\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# as they expect that the same step is used when logging epoch end metrics even when the batch loop has\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# finished. this means the attribute does not exactly track the number of optimizer steps applied.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# TODO(@carmocca): deprecate and rename so users don't get confused\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:140\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_skip()\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_run_start\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:141\u001b[0m, in \u001b[0;36mTrainingEpochLoop.on_run_start\u001b[0;34m(self, data_fetcher, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mepoch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reload_dataloader_state_dict(data_fetcher)\n\u001b[0;32m--> 141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[43m_update_dataloader_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:121\u001b[0m, in \u001b[0;36m_update_dataloader_iter\u001b[0;34m(data_fetcher, batch_idx)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m\"\"\"Attach the dataloader.\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_fetcher, DataLoaderIterDataFetcher):\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# restore iteration\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(data_fetcher)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/utilities/fetching.py:199\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_patch()\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefetching\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefetch_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/utilities/fetching.py:258\u001b[0m, in \u001b[0;36mDataFetcher.prefetching\u001b[0;34m(self, prefetch_batches)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(prefetch_batches):\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 258\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/utilities/fetching.py:300\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    298\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_fetch_start()\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_profiler(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfetch_next_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 300\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetched \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_fetch_end(batch, data)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/supporters.py:550\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;124;03m\"\"\"Fetches the next batch from multiple data loaders.\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \n\u001b[1;32m    547\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m        a collections of batch data\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader_iters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/supporters.py:562\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.request_next_batch\u001b[0;34m(loader_iters)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest_next_batch\u001b[39m(loader_iters: Union[Iterator, Sequence, Mapping]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the batch of data from multiple iterators.\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \n\u001b[1;32m    556\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;124;03m        Any: a collections of batch data\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_to_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:95\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Breaking condition\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, dtype) \u001b[38;5;129;01mand\u001b[39;00m (wrong_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m elem_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(data)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1203\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1229\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1229\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/_utils.py:434\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 434\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jovyan/ucb_mids_w207_final_project/notebooks/benchmark_src/cloud_dataset.py\", line 52, in __getitem__\n    x_arr = self.transforms(image=x_arr)[\"image\"]\n  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/albumentations/core/composition.py\", line 210, in __call__\n    data = t(force_apply=force_apply, **data)\n  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/albumentations/core/transforms_interface.py\", line 97, in __call__\n    return self.apply_with_params(params, **kwargs)\n  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/albumentations/core/transforms_interface.py\", line 112, in apply_with_params\n    res[key] = target_function(arg, **dict(params, **target_dependencies))\n  File \"/srv/conda/envs/notebook/lib/python3.8/site-packages/albumentations/augmentations/transforms.py\", line 2968, in apply\n    raise TypeError(\"ColorJitter transformation expects 1-channel or 3-channel images.\")\nTypeError: ColorJitter transformation expects 1-channel or 3-channel images.\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "trainer.fit(model = cloud_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "submission_assets_dir = submission_dir / \"assets\"\n",
    "submission_assets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_weight_path = submission_assets_dir / \"cloud_model.pt\"\n",
    "torch.save(cloud_model.state_dict(), model_weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### Step 7: Model Evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file benchmark_src/main.py\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import typer\n",
    "\n",
    "try:\n",
    "    from cloud_dataset import CloudDataset\n",
    "    from cloud_model import CloudModel\n",
    "except ImportError:\n",
    "    from benchmark_src.cloud_dataset import CloudDataset\n",
    "    from benchmark_src.cloud_model import CloudModel\n",
    "\n",
    "\n",
    "ROOT_DIRECTORY = Path(\"/codeexecution\")\n",
    "PREDICTIONS_DIRECTORY = ROOT_DIRECTORY / \"predictions\"\n",
    "ASSETS_DIRECTORY = ROOT_DIRECTORY / \"assets\"\n",
    "DATA_DIRECTORY = ROOT_DIRECTORY / \"data\"\n",
    "INPUT_IMAGES_DIRECTORY = DATA_DIRECTORY / \"test_features\"\n",
    "\n",
    "# Set the pytorch cache directory and include cached models in your submission.zip\n",
    "os.environ[\"TORCH_HOME\"] = str(ASSETS_DIRECTORY / \"assets/torch\")\n",
    "\n",
    "\n",
    "def get_metadata(features_dir: os.PathLike, bands: List[str]):\n",
    "    \"\"\"\n",
    "    Given a folder of feature data, return a dataframe where the index is the chip id\n",
    "    and there is a column for the path to each band's TIF image.\n",
    "\n",
    "    Args:\n",
    "        features_dir (os.PathLike): path to the directory of feature data, which should have\n",
    "            a folder for each chip\n",
    "        bands (list[str]): list of bands provided for each chip\n",
    "    \"\"\"\n",
    "    chip_metadata = pd.DataFrame(index=[f\"{band}_path\" for band in bands])\n",
    "    chip_ids = (\n",
    "        pth.name for pth in features_dir.iterdir() if not pth.name.startswith(\".\")\n",
    "    )\n",
    "\n",
    "    for chip_id in chip_ids:\n",
    "        chip_bands = [features_dir / chip_id / f\"{band}.tif\" for band in bands]\n",
    "        chip_metadata[chip_id] = chip_bands\n",
    "\n",
    "    return chip_metadata.transpose().reset_index().rename(columns={\"index\": \"chip_id\"})\n",
    "\n",
    "\n",
    "def make_predictions(\n",
    "    model: CloudModel,\n",
    "    x_paths: pd.DataFrame,\n",
    "    bands: List[str],\n",
    "    predictions_dir: os.PathLike,\n",
    "):\n",
    "    \"\"\"Predicts cloud cover and saves results to the predictions directory.\n",
    "\n",
    "    Args:\n",
    "        model (CloudModel): an instantiated CloudModel based on pl.LightningModule\n",
    "        x_paths (pd.DataFrame): a dataframe with a row for each chip. There must be a column for chip_id,\n",
    "                and a column with the path to the TIF for each of bands provided\n",
    "        bands (list[str]): list of bands provided for each chip\n",
    "        predictions_dir (os.PathLike): Destination directory to save the predicted TIF masks\n",
    "    \"\"\"\n",
    "    test_dataset = CloudDataset(x_paths=x_paths, bands=bands)\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=model.batch_size,\n",
    "        num_workers=model.num_workers,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    for batch_index, batch in enumerate(test_dataloader):\n",
    "        logger.debug(f\"Predicting batch {batch_index} of {len(test_dataloader)}\")\n",
    "        x = batch[\"chip\"]\n",
    "        preds = model.forward(x)\n",
    "        preds = torch.softmax(preds, dim=1)[:, 1]\n",
    "        preds = (preds > 0.5).detach().numpy().astype(\"uint8\")\n",
    "        for chip_id, pred in zip(batch[\"chip_id\"], preds):\n",
    "            chip_pred_path = predictions_dir / f\"{chip_id}.tif\"\n",
    "            chip_pred_im = Image.fromarray(pred)\n",
    "            chip_pred_im.save(chip_pred_path)\n",
    "\n",
    "\n",
    "def main(\n",
    "    model_weights_path: Path = ASSETS_DIRECTORY / \"cloud_model.pt\",\n",
    "    test_features_dir: Path = DATA_DIRECTORY / \"test_features\",\n",
    "    predictions_dir: Path = PREDICTIONS_DIRECTORY,\n",
    "    bands: List[str] = [\"B02\", \"B03\", \"B04\", \"B08\"],\n",
    "    fast_dev_run: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate predictions for the chips in test_features_dir using the model saved at\n",
    "    model_weights_path.\n",
    "\n",
    "    Predictions are saved in predictions_dir. The default paths to all three files are based on\n",
    "    the structure of the code execution runtime.\n",
    "\n",
    "    Args:\n",
    "        model_weights_path (os.PathLike): Path to the weights of a trained CloudModel.\n",
    "        test_features_dir (os.PathLike, optional): Path to the features for the test data. Defaults\n",
    "            to 'data/test_features' in the same directory as main.py\n",
    "        predictions_dir (os.PathLike, optional): Destination directory to save the predicted TIF masks\n",
    "            Defaults to 'predictions' in the same directory as main.py\n",
    "        bands (List[str], optional): List of bands provided for each chip\n",
    "    \"\"\"\n",
    "    if not test_features_dir.exists():\n",
    "        raise ValueError(\n",
    "            f\"The directory for test feature images must exist and {test_features_dir} does not exist\"\n",
    "        )\n",
    "    predictions_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    logger.info(\"Loading model\")\n",
    "    model = CloudModel(bands=bands, hparams={\"weights\": None})\n",
    "    model.load_state_dict(torch.load(model_weights_path))\n",
    "\n",
    "    logger.info(\"Loading test metadata\")\n",
    "    test_metadata = get_metadata(test_features_dir, bands=bands)\n",
    "    if fast_dev_run:\n",
    "        test_metadata = test_metadata.head()\n",
    "    logger.info(f\"Found {len(test_metadata)} chips\")\n",
    "\n",
    "    logger.info(\"Generating predictions in batches\")\n",
    "    make_predictions(model, test_metadata, bands, predictions_dir)\n",
    "\n",
    "    logger.info(f\"\"\"Saved {len(list(predictions_dir.glob(\"*.tif\")))} predictions\"\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    typer.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_src.main import main\n",
    "\n",
    "main(\n",
    "    model_weights_path=submission_dir / \"assets/cloud_model.pt\",\n",
    "    test_features_dir=TRAIN_FEATURES,\n",
    "    predictions_dir=submission_dir / \"predictions\",\n",
    "    fast_dev_run=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
