{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    " ## Cloud Cover Semantic Segmentation\n",
    "### Final project: SegFormer Variant\n",
    "\n",
    "Project Team: <br>\n",
    "Kurt Eulau <br>\n",
    "Steve Hewitt <br>\n",
    "Tom Welsh <br>\n",
    "\n",
    "\n",
    "Citations: <br>\n",
    " - TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives: \n",
    " - TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1: Import packages\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas_path pytorch_lightning cloudpathlib loguru typer wandb\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_path import path  # noqa\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import rasterio\n",
    "import pyproj\n",
    "import rioxarray\n",
    "import xrspatial.multispectral as ms\n",
    "from my_preprocessing import train_val_test_split\n",
    "import segmentation_models_pytorch as smp\n",
    "from typing import Optional, List\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><h1>Added import for Segformer module</h1></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /srv/conda/envs/notebook/lib/python3.8/site-packages (4.17.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: sacremoses in /srv/conda/envs/notebook/lib/python3.8/site-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from transformers) (0.11.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from transformers) (1.20.0)\n",
      "Requirement already satisfied: filelock in /srv/conda/envs/notebook/lib/python3.8/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in /srv/conda/envs/notebook/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from requests->transformers) (2.0.11)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: click in /srv/conda/envs/notebook/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: six in /srv/conda/envs/notebook/lib/python3.8/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /srv/conda/envs/notebook/lib/python3.8/site-packages (from sacremoses->transformers) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "from transformers import SegformerForSemanticSegmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1A (Optional): Log training to wandb.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"segformer-model-v1\", entity=\"w207-clouds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: Define working directories and global variables\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/driven-data/cloud-cover\")\n",
    "TRAIN_FEATURES = DATA_DIR / \"train_features\"\n",
    "TRAIN_LABELS = DATA_DIR / \"train_labels\"\n",
    "\n",
    "assert TRAIN_FEATURES.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDS = [\"B02\", \"B03\", \"B04\", \"B08\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chip_id</th>\n",
       "      <th>location</th>\n",
       "      <th>datetime</th>\n",
       "      <th>cloudpath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adwp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adwu</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adwz</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adxp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adxp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aeaj</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/aeaj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chip_id  location              datetime                   cloudpath\n",
       "0    adwp  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwp\n",
       "1    adwu  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwu\n",
       "2    adwz  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwz\n",
       "3    adxp  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adxp\n",
       "4    aeaj  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/aeaj"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta = pd.read_csv(DATA_DIR / \"train_metadata.csv\")\n",
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create benchmark_src folder\n",
    "submission_dir = Path(\"benchmark_src\")\n",
    "if submission_dir.exists():\n",
    "    shutil.rmtree(submission_dir)\n",
    "\n",
    "submission_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Create scripts to define loss function and classes.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing benchmark_src/losses.py\n"
     ]
    }
   ],
   "source": [
    "%%file {submission_dir}/losses.py\n",
    "import numpy as np\n",
    "\n",
    "def intersection_over_union(pred, true):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates intersection and union for a batch of images.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): a tensor of predictions\n",
    "        true (torc.Tensor): a tensor of labels\n",
    "\n",
    "    Returns:\n",
    "        intersection (int): total intersection of pixels\n",
    "        union (int): total union of pixels\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    valid_pixel_mask = true.ne(255)  # valid pixel mask\n",
    "    true = true.masked_select(valid_pixel_mask).to(\"cpu\")\n",
    "    pred = pred.masked_select(valid_pixel_mask).to(\"cpu\")\n",
    "\n",
    "    # Intersection and union totals\n",
    "    intersection = np.logical_and(true, pred)\n",
    "    union = np.logical_or(true, pred)\n",
    "    return intersection.sum() / union.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing benchmark_src/cloud_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%file {submission_dir}/cloud_dataset.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import torch\n",
    "from typing import Optional, List\n",
    "\n",
    "class CloudDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Reads in images, transforms pixel values, and serves a\n",
    "    dictionary containing chip ids, image tensors, and\n",
    "    label masks (where available).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_paths: pd.DataFrame, # : syntax specifies datatype for each parameter for CloudDataset objects\n",
    "        bands: List[str],\n",
    "        y_paths: Optional[pd.DataFrame] = None,\n",
    "        transforms: Optional[list] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiate the CloudDataset class.\n",
    "\n",
    "        Args:\n",
    "            x_paths (pd.DataFrame): a dataframe with a row for each chip. There must be a column for chip_id,\n",
    "                and a column with the path to the TIF for each of bands\n",
    "            bands (list[str]): list of the bands included in the data\n",
    "            y_paths (pd.DataFrame, optional): a dataframe with a for each chip and columns for chip_id\n",
    "                and the path to the label TIF with ground truth cloud cover\n",
    "            transforms (list, optional): list of transforms to apply to the feature data (eg augmentations)\n",
    "        \"\"\"\n",
    "        self.data = x_paths\n",
    "        self.label = y_paths\n",
    "        self.transforms = transforms\n",
    "        self.bands = bands\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # Similar to pop, helps iterate through dataset\n",
    "    def __getitem__(self, idx: int):\n",
    "        # Loads an n-channel image from a chip-level dataframe\n",
    "        img = self.data.loc[idx]\n",
    "        band_arrs = []\n",
    "        for band in self.bands:\n",
    "            with rasterio.open(img[f\"{band}_path\"]) as b:\n",
    "                band_arr = b.read(1).astype(\"float32\")\n",
    "            band_arrs.append(band_arr)\n",
    "        x_arr = np.stack(band_arrs, axis=-1) # 3-dimensional array\n",
    "\n",
    "        # Apply data augmentations, if provided\n",
    "        if self.transforms:\n",
    "            x_arr = self.transforms(image=x_arr)[\"image\"]\n",
    "        x_arr = np.transpose(x_arr, [2, 0, 1]) # re-orders array to match expected format needed for model\n",
    "\n",
    "        # Prepare dictionary for item\n",
    "        item = {\"chip_id\": img.chip_id, \"chip\": x_arr}\n",
    "\n",
    "        # Load label if available\n",
    "        if self.label is not None:\n",
    "            label_path = self.label.loc[idx].label_path\n",
    "            with rasterio.open(label_path) as lp:\n",
    "                y_arr = lp.read(1).astype(\"float32\")\n",
    "            # Apply same data augmentations to the label\n",
    "            if self.transforms:\n",
    "                y_arr = self.transforms(image=y_arr)[\"image\"]\n",
    "            item[\"label\"] = y_arr\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><h1>Changing model to SegFormer and removing unused hyperparameters</h1></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing benchmark_src/cloud_model.py\n"
     ]
    }
   ],
   "source": [
    "%%file {submission_dir}/cloud_model.py\n",
    "from typing import Optional, List\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "from benchmark_src.cloud_dataset import CloudDataset\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "try:\n",
    "    from cloud_dataset import CloudDataset\n",
    "    from losses import intersection_over_union\n",
    "except ImportError:\n",
    "    from benchmark_src.cloud_dataset import CloudDataset\n",
    "    from benchmark_src.losses import intersection_over_union\n",
    "\n",
    "class CloudModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bands: List[str],\n",
    "        x_train: Optional[pd.DataFrame] = None,\n",
    "        y_train: Optional[pd.DataFrame] = None,\n",
    "        x_val: Optional[pd.DataFrame] = None,\n",
    "        y_val: Optional[pd.DataFrame] = None,\n",
    "        hparams: dict = {},\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiate the CloudModel class based on the pl.LightningModule\n",
    "        (https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html).\n",
    "\n",
    "        Args:\n",
    "            bands (list[str]): Names of the bands provided for each chip\n",
    "            x_train (pd.DataFrame, optional): a dataframe of the training features with a row for each chip.\n",
    "                There must be a column for chip_id, and a column with the path to the TIF for each of bands.\n",
    "                Required for model training\n",
    "            y_train (pd.DataFrame, optional): a dataframe of the training labels with a for each chip\n",
    "                and columns for chip_id and the path to the label TIF with ground truth cloud cover.\n",
    "                Required for model training\n",
    "            x_val (pd.DataFrame, optional): a dataframe of the validation features with a row for each chip.\n",
    "                There must be a column for chip_id, and a column with the path to the TIF for each of bands.\n",
    "                Required for model training\n",
    "            y_val (pd.DataFrame, optional): a dataframe of the validation labels with a for each chip\n",
    "                and columns for chip_id and the path to the label TIF with ground truth cloud cover.\n",
    "                Required for model training\n",
    "            hparams (dict, optional): Dictionary of additional modeling parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hparams.update(hparams)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # required\n",
    "        self.bands = bands\n",
    "\n",
    "        # optional modeling params\n",
    "        #self.backbone = self.hparams.get(\"backbone\", \"resnet34\")\n",
    "        #self.weights = self.hparams.get(\"weights\", \"imagenet\")\n",
    "        self.learning_rate = self.hparams.get(\"lr\", 1e-3)\n",
    "        self.patience = self.hparams.get(\"patience\", 4)\n",
    "        self.num_workers = self.hparams.get(\"num_workers\", 2)\n",
    "        self.batch_size = self.hparams.get(\"batch_size\", 16)\n",
    "        self.gpu = self.hparams.get(\"gpu\", False)\n",
    "        self.transform = None\n",
    "        self.bce = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "        # Instantiate datasets, model, and trainer params if provided\n",
    "        self.train_dataset = CloudDataset(\n",
    "            x_paths=x_train,\n",
    "            bands=self.bands,\n",
    "            y_paths=y_train,\n",
    "            transforms=self.transform,\n",
    "        )\n",
    "        self.val_dataset = CloudDataset(\n",
    "            x_paths=x_val,\n",
    "            bands=self.bands,\n",
    "            y_paths=y_val,\n",
    "            transforms=None,\n",
    "        )\n",
    "        self.model = self._prepare_model()\n",
    "\n",
    "    ## Required LightningModule methods ##\n",
    "\n",
    "    def forward(self, image: torch.Tensor):\n",
    "        # Forward pass\n",
    "        return self.model(image)\n",
    "\n",
    "    def training_step(self, batch: dict, batch_idx: int):\n",
    "        \"\"\"\n",
    "        Training step.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): dictionary of items from CloudDataset of the form\n",
    "                {'chip_id': list[str], 'chip': list[torch.Tensor], 'label': list[torch.Tensor]}\n",
    "            batch_idx (int): batch number\n",
    "        \"\"\"\n",
    "        if self.train_dataset.data is None:\n",
    "            raise ValueError(\n",
    "                \"x_train and y_train must be specified when CloudModel is instantiated to run training\"\n",
    "            )\n",
    "\n",
    "        # Switch on training mode\n",
    "        self.model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        y = batch[\"label\"].long()\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "        # Forward pass\n",
    "        preds = self.model(x)\n",
    "        preds = torch.nn.functional.interpolate(preds.logits, size=y.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        preds = preds.argmax(dim=1)\n",
    "        #preds = self(x)\n",
    "\n",
    "        # Log batch loss\n",
    "        #loss = torch.nn.CrossEntropyLoss(reduction=\"none\")(preds, y).mean()\n",
    "        #loss = torch.nn.BCEWithLogitsLoss(reduction=\"none\", requires_grad=True)(preds.float(), y.float()).mean()\n",
    "        #loss = preds.loss(y)\n",
    "        loss = self.bce(preds.float(), y.float())\n",
    "        \n",
    "        self.log(\n",
    "            \"loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: dict, batch_idx: int):\n",
    "        \"\"\"\n",
    "        Validation step.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): dictionary of items from CloudDataset of the form\n",
    "                {'chip_id': list[str], 'chip': list[torch.Tensor], 'label': list[torch.Tensor]}\n",
    "            batch_idx (int): batch number\n",
    "        \"\"\"\n",
    "        if self.val_dataset.data is None:\n",
    "            raise ValueError(\n",
    "                \"x_val and y_val must be specified when CloudModel is instantiated to run validation\"\n",
    "            )\n",
    "\n",
    "        # Switch on validation mode\n",
    "        self.model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        y = batch[\"label\"].long()\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "        # Forward pass & softmax\n",
    "        #preds = self.forward(x)\n",
    "        #preds = torch.softmax(preds, dim=1)[:, 1]\n",
    "        #preds = preds.\n",
    "        #preds = (preds > 0.5) * 1  # convert to int\n",
    "        \n",
    "        preds = self.model(x)\n",
    "        preds = torch.nn.functional.interpolate(preds.logits, size=y.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        preds = preds.argmax(dim=1)\n",
    "        #preds = torch.nn.functional.interpolate(preds, x.shape[-2:], mode=\"bilinear\", align_corners=True)\n",
    "        #preds = preds.squeeze(1)\n",
    "\n",
    "        # Log batch IOU\n",
    "        batch_iou = intersection_over_union(preds, y)\n",
    "        self.log(\n",
    "            \"iou\", batch_iou, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return batch_iou\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # DataLoader class for training\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # DataLoader class for validation\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=0,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\n",
    "        return [opt], [sch]\n",
    "\n",
    "    ## Convenience Methods ##\n",
    "\n",
    "    def _prepare_model(self):\n",
    "        # Using a Segformer instead of the standard Unet.\n",
    "        segf_model = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b1-finetuned-ade-512-512')\n",
    "        \n",
    "        #Chop off first layer of pre-trained model and replace it with one that can handle 4 input channels while maintaining weights.\n",
    "        weight = segf_model.segformer.encoder.patch_embeddings[0].proj.weight.clone()\n",
    "        segf_model.segformer.encoder.patch_embeddings[0].proj = torch.nn.Conv2d(4, 64, kernel_size=7, stride=4, padding=3, bias=False)\n",
    "        with torch.no_grad():\n",
    "            segf_model.segformer.encoder.patch_embeddings[0].proj.weight[:, :3] = weight\n",
    "            segf_model.segformer.encoder.patch_embeddings[0].proj.weight[:, 3] = segf_model.segformer.encoder.patch_embeddings[0].proj.weight[:, 0]\n",
    "        \n",
    "        if self.gpu:\n",
    "            segf_model.cuda()\n",
    "        \n",
    "        \n",
    "\n",
    "        return segf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 4: Define functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_paths(df, feature_dir, label_dir=None, bands=BANDS):\n",
    "    \"\"\"\n",
    "    Given dataframe with a column for chip_id, returns a dataframe with a column\n",
    "    added indicating the path to each band's TIF image as \"{band}_path\", eg \"B02_path\".\n",
    "    A column is also added to the dataframe with paths to the label TIF, if the\n",
    "    path to the labels directory is provided.\n",
    "    \"\"\"\n",
    "    for band in bands:\n",
    "        df[f\"{band}_path\"] = feature_dir / df[\"chip_id\"] / f\"{band}.tif\"\n",
    "        # make sure a random sample of paths exist\n",
    "        assert df.sample(n=40, random_state=5)[f\"{band}_path\"].path.exists().all()\n",
    "    if label_dir is not None:\n",
    "        df[\"label_path\"] = label_dir / (df[\"chip_id\"] + \".tif\")\n",
    "        # make sure a random sample of paths exist\n",
    "        assert df.sample(n=40, random_state=5)[\"label_path\"].path.exists().all()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 5: Read data and preprocess\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chip_id</th>\n",
       "      <th>location</th>\n",
       "      <th>datetime</th>\n",
       "      <th>cloudpath</th>\n",
       "      <th>B02_path</th>\n",
       "      <th>B03_path</th>\n",
       "      <th>B04_path</th>\n",
       "      <th>B08_path</th>\n",
       "      <th>label_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adwp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwp</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/adwp.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adwu</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwu</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwu/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwu/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwu/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwu/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/adwu.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adwz</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwz</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwz/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwz/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwz/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwz/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/adwz.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adxp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adxp</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adxp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adxp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adxp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adxp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/adxp.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aeaj</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/aeaj</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeaj/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeaj/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeaj/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeaj/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/aeaj.tif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chip_id  location              datetime                   cloudpath  \\\n",
       "0    adwp  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwp   \n",
       "1    adwu  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwu   \n",
       "2    adwz  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwz   \n",
       "3    adxp  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adxp   \n",
       "4    aeaj  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/aeaj   \n",
       "\n",
       "                                            B02_path  \\\n",
       "0  /driven-data/cloud-cover/train_features/adwp/B...   \n",
       "1  /driven-data/cloud-cover/train_features/adwu/B...   \n",
       "2  /driven-data/cloud-cover/train_features/adwz/B...   \n",
       "3  /driven-data/cloud-cover/train_features/adxp/B...   \n",
       "4  /driven-data/cloud-cover/train_features/aeaj/B...   \n",
       "\n",
       "                                            B03_path  \\\n",
       "0  /driven-data/cloud-cover/train_features/adwp/B...   \n",
       "1  /driven-data/cloud-cover/train_features/adwu/B...   \n",
       "2  /driven-data/cloud-cover/train_features/adwz/B...   \n",
       "3  /driven-data/cloud-cover/train_features/adxp/B...   \n",
       "4  /driven-data/cloud-cover/train_features/aeaj/B...   \n",
       "\n",
       "                                            B04_path  \\\n",
       "0  /driven-data/cloud-cover/train_features/adwp/B...   \n",
       "1  /driven-data/cloud-cover/train_features/adwu/B...   \n",
       "2  /driven-data/cloud-cover/train_features/adwz/B...   \n",
       "3  /driven-data/cloud-cover/train_features/adxp/B...   \n",
       "4  /driven-data/cloud-cover/train_features/aeaj/B...   \n",
       "\n",
       "                                            B08_path  \\\n",
       "0  /driven-data/cloud-cover/train_features/adwp/B...   \n",
       "1  /driven-data/cloud-cover/train_features/adwu/B...   \n",
       "2  /driven-data/cloud-cover/train_features/adwz/B...   \n",
       "3  /driven-data/cloud-cover/train_features/adxp/B...   \n",
       "4  /driven-data/cloud-cover/train_features/aeaj/B...   \n",
       "\n",
       "                                       label_path  \n",
       "0  /driven-data/cloud-cover/train_labels/adwp.tif  \n",
       "1  /driven-data/cloud-cover/train_labels/adwu.tif  \n",
       "2  /driven-data/cloud-cover/train_labels/adwz.tif  \n",
       "3  /driven-data/cloud-cover/train_labels/adxp.tif  \n",
       "4  /driven-data/cloud-cover/train_labels/aeaj.tif  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta = add_paths(train_meta, TRAIN_FEATURES, TRAIN_LABELS)\n",
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = train_val_test_split(train_meta, column='location', pct_train=0.6, pct_val=0.2, pct_test=0.2, random_state=42)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7017, 9), (2349, 9), (2382, 9))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features from labels\n",
    "feature_cols = [\"chip_id\"] + [f\"{band}_path\" for band in BANDS]\n",
    "\n",
    "train_x = train[feature_cols].copy()\n",
    "train_y = train[[\"chip_id\", \"label_path\"]].copy()\n",
    "\n",
    "val_x = val[feature_cols].copy()\n",
    "val_y = val[[\"chip_id\", \"label_path\"]].copy()\n",
    "\n",
    "test_x = test[feature_cols].copy()\n",
    "test_y = test[[\"chip_id\", \"label_path\"]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 6: Model Training\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "## Set up pytorch_lightning.Trainer object\n",
    "from benchmark_src.cloud_model import CloudModel\n",
    "\n",
    "cloud_model = CloudModel(\n",
    "    bands=BANDS,\n",
    "    x_train=train_x,\n",
    "    y_train=train_y,\n",
    "    x_val=val_x,\n",
    "    y_val=val_y,\n",
    "    hparams={\"num_workers\": 7, \"batch_size\": 16},\n",
    ")\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"iou_epoch\", mode=\"max\", verbose=True\n",
    ")\n",
    "early_stopping_callback = pl.callbacks.early_stopping.EarlyStopping(\n",
    "    monitor=\"iou_epoch\",\n",
    "    patience=(cloud_model.patience * 3),\n",
    "    mode=\"max\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    fast_dev_run=False,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    logger = wandb_logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cloud_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mw251_final_project\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/ucb_mids_w207_final_project/notebooks/wandb/run-20220320_021555-3n3q6f68</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/w207-clouds/segformer-model-v1/runs/3n3q6f68\" target=\"_blank\">iconic-jazz-16</a></strong> to <a href=\"https://wandb.ai/w207-clouds/segformer-model-v1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type DataFrame that is 3122709 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type DataFrame that is 1101813 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type DataFrame that is 1045449 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type DataFrame that is 368937 bytes\n",
      "\n",
      "  | Name  | Type                             | Params\n",
      "-----------------------------------------------------------\n",
      "0 | bce   | BCEWithLogitsLoss                | 0     \n",
      "1 | model | SegformerForSemanticSegmentation | 13.7 M\n",
      "-----------------------------------------------------------\n",
      "13.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "13.7 M    Total params\n",
      "54.875    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e920da2a3324d7f961eae39fb78784c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcloud_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:740\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    735\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m     )\n\u001b[1;32m    739\u001b[0m     train_dataloaders \u001b[38;5;241m=\u001b[39m train_dataloader\n\u001b[0;32m--> 740\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:685\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;124;03mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;124;03mas all errors should funnel through them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:777\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;66;03m# TODO: ckpt_path only in v1.7\u001b[39;00m\n\u001b[1;32m    776\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m--> 777\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1199\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;66;03m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[0;32m-> 1199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;66;03m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_dispatch()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1279\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_type_plugin\u001b[38;5;241m.\u001b[39mstart_predicting(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1279\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_type_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:202\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_training\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1289\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1319\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1319\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:234\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m data_fetcher \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mget_profiled_dataloader(dataloader)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# the global step is manually decreased here due to backwards compatibility with existing loggers\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# as they expect that the same step is used when logging epoch end metrics even when the batch loop has\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# finished. this means the attribute does not exactly track the number of optimizer steps applied.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# TODO(@carmocca): deprecate and rename so users don't get confused\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:193\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 193\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m     87\u001b[0m     optimizers \u001b[38;5;241m=\u001b[39m _get_active_optimizers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39moptimizers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39moptimizer_frequencies, batch_idx)\n\u001b[0;32m---> 88\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_loop\u001b[38;5;241m.\u001b[39mrun(split_batch, batch_idx)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:215\u001b[0m, in \u001b[0;36mOptimizerLoop.advance\u001b[0;34m(self, batch, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madvance\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_optimization\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim_progress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_position\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;66;03m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[39;00m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;66;03m# would be skipped otherwise\u001b[39;00m\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_idx] \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39masdict()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:266\u001b[0m, in \u001b[0;36mOptimizerLoop._run_optimization\u001b[0;34m(self, split_batch, batch_idx, optimizer, opt_idx)\u001b[0m\n\u001b[1;32m    259\u001b[0m         closure()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;66;03m# if no result, user decided to skip optimization\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# otherwise update running loss + reset accumulated loss\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# TODO: find proper way to handle updating running loss\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:378\u001b[0m, in \u001b[0;36mOptimizerLoop._optimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m \u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_tpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_device_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDeviceType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTPU\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_TPU_AVAILABLE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_native_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamp_backend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamp_backend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mAMPType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNATIVE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_lbfgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_lbfgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:1652\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1572\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1573\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1580\u001b[0m     using_lbfgs: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1581\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;124;03m    Override this method to adjust the default way the\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;124;03m    :class:`~pytorch_lightning.trainer.trainer.Trainer` calls each optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1650\u001b[0m \n\u001b[1;32m   1651\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1652\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:164\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m trainer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(profiler_action):\n\u001b[0;32m--> 164\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py:336\u001b[0m, in \u001b[0;36mAccelerator.optimizer_step\u001b[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m\"\"\"performs the actual optimizer step.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m    **kwargs: Any extra arguments to ``optimizer.step``\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    335\u001b[0m model \u001b[38;5;241m=\u001b[39m model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\n\u001b[0;32m--> 336\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:163\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, model, optimizer, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[1;32m    162\u001b[0m     closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001b[0;32m--> 163\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/optim/adam.py:92\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m---> 92\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m     95\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:148\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[0;34m(self, model, optimizer, optimizer_idx, closure)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    137\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m    141\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer, optimizer_idx)\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:160\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:155\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 155\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:327\u001b[0m, in \u001b[0;36mOptimizerLoop._make_backward_fn.<locals>.backward_fn\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward_fn\u001b[39m(loss: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;66;03m# check if model weights are nan\u001b[39;00m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_terminate_on_nan:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py:311\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, closure_loss, *args, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_type_plugin\u001b[38;5;241m.\u001b[39mpre_backward(closure_loss)\n\u001b[1;32m    309\u001b[0m closure_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpre_backward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, closure_loss)\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m closure_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpost_backward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, closure_loss)\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_type_plugin\u001b[38;5;241m.\u001b[39mpost_backward(closure_loss)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:91\u001b[0m, in \u001b[0;36mPrecisionPlugin.backward\u001b[0;34m(self, model, closure_loss, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# do backward pass\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[0;32m---> 91\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_backward(closure_loss, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:1434\u001b[0m, in \u001b[0;36mLightningModule.backward\u001b[0;34m(self, loss, optimizer, optimizer_idx, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\n\u001b[1;32m   1418\u001b[0m     \u001b[38;5;28mself\u001b[39m, loss: Tensor, optimizer: Optional[Optimizer], optimizer_idx: Optional[\u001b[38;5;28mint\u001b[39m], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1419\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1420\u001b[0m     \u001b[38;5;124;03m\"\"\"Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your\u001b[39;00m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;124;03m    own implementation if you need to.\u001b[39;00m\n\u001b[1;32m   1422\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1432\u001b[0m \u001b[38;5;124;03m            loss.backward()\u001b[39;00m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1434\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "trainer.fit(model = cloud_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "submission_assets_dir = submission_dir / \"assets\"\n",
    "submission_assets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_weight_path = submission_assets_dir / \"cloud_model.pt\"\n",
    "torch.save(cloud_model.state_dict(), model_weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 7: Model Evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file benchmark_src/main.py\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import typer\n",
    "\n",
    "try:\n",
    "    from cloud_dataset import CloudDataset\n",
    "    from cloud_model import CloudModel\n",
    "except ImportError:\n",
    "    from benchmark_src.cloud_dataset import CloudDataset\n",
    "    from benchmark_src.cloud_model import CloudModel\n",
    "\n",
    "\n",
    "ROOT_DIRECTORY = Path(\"/codeexecution\")\n",
    "PREDICTIONS_DIRECTORY = ROOT_DIRECTORY / \"predictions\"\n",
    "ASSETS_DIRECTORY = ROOT_DIRECTORY / \"assets\"\n",
    "DATA_DIRECTORY = ROOT_DIRECTORY / \"data\"\n",
    "INPUT_IMAGES_DIRECTORY = DATA_DIRECTORY / \"test_features\"\n",
    "\n",
    "# Set the pytorch cache directory and include cached models in your submission.zip\n",
    "os.environ[\"TORCH_HOME\"] = str(ASSETS_DIRECTORY / \"assets/torch\")\n",
    "\n",
    "\n",
    "def get_metadata(features_dir: os.PathLike, bands: List[str]):\n",
    "    \"\"\"\n",
    "    Given a folder of feature data, return a dataframe where the index is the chip id\n",
    "    and there is a column for the path to each band's TIF image.\n",
    "\n",
    "    Args:\n",
    "        features_dir (os.PathLike): path to the directory of feature data, which should have\n",
    "            a folder for each chip\n",
    "        bands (list[str]): list of bands provided for each chip\n",
    "    \"\"\"\n",
    "    chip_metadata = pd.DataFrame(index=[f\"{band}_path\" for band in bands])\n",
    "    chip_ids = (\n",
    "        pth.name for pth in features_dir.iterdir() if not pth.name.startswith(\".\")\n",
    "    )\n",
    "\n",
    "    for chip_id in chip_ids:\n",
    "        chip_bands = [features_dir / chip_id / f\"{band}.tif\" for band in bands]\n",
    "        chip_metadata[chip_id] = chip_bands\n",
    "\n",
    "    return chip_metadata.transpose().reset_index().rename(columns={\"index\": \"chip_id\"})\n",
    "\n",
    "\n",
    "def make_predictions(\n",
    "    model: CloudModel,\n",
    "    x_paths: pd.DataFrame,\n",
    "    bands: List[str],\n",
    "    predictions_dir: os.PathLike,\n",
    "):\n",
    "    \"\"\"Predicts cloud cover and saves results to the predictions directory.\n",
    "\n",
    "    Args:\n",
    "        model (CloudModel): an instantiated CloudModel based on pl.LightningModule\n",
    "        x_paths (pd.DataFrame): a dataframe with a row for each chip. There must be a column for chip_id,\n",
    "                and a column with the path to the TIF for each of bands provided\n",
    "        bands (list[str]): list of bands provided for each chip\n",
    "        predictions_dir (os.PathLike): Destination directory to save the predicted TIF masks\n",
    "    \"\"\"\n",
    "    test_dataset = CloudDataset(x_paths=x_paths, bands=bands)\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=model.batch_size,\n",
    "        num_workers=model.num_workers,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    for batch_index, batch in enumerate(test_dataloader):\n",
    "        logger.debug(f\"Predicting batch {batch_index} of {len(test_dataloader)}\")\n",
    "        x = batch[\"chip\"]\n",
    "        preds = model.forward(x)\n",
    "        preds = torch.softmax(preds, dim=1)[:, 1]\n",
    "        preds = (preds > 0.5).detach().numpy().astype(\"uint8\")\n",
    "        for chip_id, pred in zip(batch[\"chip_id\"], preds):\n",
    "            chip_pred_path = predictions_dir / f\"{chip_id}.tif\"\n",
    "            chip_pred_im = Image.fromarray(pred)\n",
    "            chip_pred_im.save(chip_pred_path)\n",
    "\n",
    "\n",
    "def main(\n",
    "    model_weights_path: Path = ASSETS_DIRECTORY / \"cloud_model.pt\",\n",
    "    test_features_dir: Path = DATA_DIRECTORY / \"test_features\",\n",
    "    predictions_dir: Path = PREDICTIONS_DIRECTORY,\n",
    "    bands: List[str] = [\"B02\", \"B03\", \"B04\", \"B08\"],\n",
    "    fast_dev_run: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate predictions for the chips in test_features_dir using the model saved at\n",
    "    model_weights_path.\n",
    "\n",
    "    Predictions are saved in predictions_dir. The default paths to all three files are based on\n",
    "    the structure of the code execution runtime.\n",
    "\n",
    "    Args:\n",
    "        model_weights_path (os.PathLike): Path to the weights of a trained CloudModel.\n",
    "        test_features_dir (os.PathLike, optional): Path to the features for the test data. Defaults\n",
    "            to 'data/test_features' in the same directory as main.py\n",
    "        predictions_dir (os.PathLike, optional): Destination directory to save the predicted TIF masks\n",
    "            Defaults to 'predictions' in the same directory as main.py\n",
    "        bands (List[str], optional): List of bands provided for each chip\n",
    "    \"\"\"\n",
    "    if not test_features_dir.exists():\n",
    "        raise ValueError(\n",
    "            f\"The directory for test feature images must exist and {test_features_dir} does not exist\"\n",
    "        )\n",
    "    predictions_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    logger.info(\"Loading model\")\n",
    "    model = CloudModel(bands=bands, hparams={\"weights\": None})\n",
    "    model.load_state_dict(torch.load(model_weights_path))\n",
    "\n",
    "    logger.info(\"Loading test metadata\")\n",
    "    test_metadata = get_metadata(test_features_dir, bands=bands)\n",
    "    if fast_dev_run:\n",
    "        test_metadata = test_metadata.head()\n",
    "    logger.info(f\"Found {len(test_metadata)} chips\")\n",
    "\n",
    "    logger.info(\"Generating predictions in batches\")\n",
    "    make_predictions(model, test_metadata, bands, predictions_dir)\n",
    "\n",
    "    logger.info(f\"\"\"Saved {len(list(predictions_dir.glob(\"*.tif\")))} predictions\"\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    typer.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_src.main import main\n",
    "\n",
    "main(\n",
    "    model_weights_path=submission_dir / \"assets/cloud_model.pt\",\n",
    "    test_features_dir=TRAIN_FEATURES,\n",
    "    predictions_dir=submission_dir / \"predictions\",\n",
    "    fast_dev_run=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
