{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas_path pytorch_lightning cloudpathlib loguru typer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_path import path  # noqa\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import rasterio\n",
    "import pyproj\n",
    "import rioxarray\n",
    "import xrspatial.multispectral as ms\n",
    "from my_preprocessing import train_val_test_split\n",
    "from typing import Optional, List\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working directories and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/driven-data/cloud-cover\")\n",
    "TRAIN_FEATURES = DATA_DIR / \"train_features\"\n",
    "TRAIN_LABELS = DATA_DIR / \"train_labels\"\n",
    "\n",
    "assert TRAIN_FEATURES.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDS = [\"B02\", \"B03\", \"B04\", \"B08\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chip_id</th>\n",
       "      <th>location</th>\n",
       "      <th>datetime</th>\n",
       "      <th>cloudpath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adwp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adwu</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adwz</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adxp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adxp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aeaj</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/aeaj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chip_id  location              datetime                   cloudpath\n",
       "0    adwp  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwp\n",
       "1    adwu  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwu\n",
       "2    adwz  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwz\n",
       "3    adxp  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adxp\n",
       "4    aeaj  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/aeaj"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta = pd.read_csv(DATA_DIR / \"train_metadata.csv\")\n",
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create benchmark_src folder\n",
    "submission_dir = Path(\"benchmark_src\")\n",
    "if submission_dir.exists():\n",
    "    shutil.rmtree(submission_dir)\n",
    "\n",
    "submission_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing benchmark_src/cloud_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%file {submission_dir}/cloud_dataset.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import torch\n",
    "from typing import Optional, List\n",
    "\n",
    "\n",
    "class CloudDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Reads in images, transforms pixel values, and serves a\n",
    "    dictionary containing chip ids, image tensors, and\n",
    "    label masks (where available).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_paths: pd.DataFrame, # : syntax specifies datatype for each parameter for CloudDataset objects\n",
    "        bands: List[str],\n",
    "        y_paths: Optional[pd.DataFrame] = None,\n",
    "        transforms: Optional[list] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiate the CloudDataset class.\n",
    "\n",
    "        Args:\n",
    "            x_paths (pd.DataFrame): a dataframe with a row for each chip. There must be a column for chip_id,\n",
    "                and a column with the path to the TIF for each of bands\n",
    "            bands (list[str]): list of the bands included in the data\n",
    "            y_paths (pd.DataFrame, optional): a dataframe with a for each chip and columns for chip_id\n",
    "                and the path to the label TIF with ground truth cloud cover\n",
    "            transforms (list, optional): list of transforms to apply to the feature data (eg augmentations)\n",
    "        \"\"\"\n",
    "        self.data = x_paths\n",
    "        self.label = y_paths\n",
    "        self.transforms = transforms\n",
    "        self.bands = bands\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # Similar to pop, helps iterate through dataset\n",
    "    def __getitem__(self, idx: int):\n",
    "        # Loads an n-channel image from a chip-level dataframe\n",
    "        img = self.data.loc[idx]\n",
    "        band_arrs = []\n",
    "        for band in self.bands:\n",
    "            with rasterio.open(img[f\"{band}_path\"]) as b:\n",
    "                band_arr = b.read(1).astype(\"float32\")\n",
    "            band_arrs.append(band_arr)\n",
    "        x_arr = np.stack(band_arrs, axis=-1) # 3-dimensional array\n",
    "\n",
    "        # Apply data augmentations, if provided\n",
    "        if self.transforms:\n",
    "            x_arr = self.transforms(image=x_arr)[\"image\"]\n",
    "        x_arr = np.transpose(x_arr, [2, 0, 1]) # re-orders array to match expected format needed for model\n",
    "\n",
    "        # Prepare dictionary for item\n",
    "        item = {\"chip_id\": img.chip_id, \"chip\": x_arr}\n",
    "\n",
    "        # Load label if available\n",
    "        if self.label is not None:\n",
    "            label_path = self.label.loc[idx].label_path\n",
    "            with rasterio.open(label_path) as lp:\n",
    "                y_arr = lp.read(1).astype(\"float32\")\n",
    "            # Apply same data augmentations to the label\n",
    "            if self.transforms:\n",
    "                y_arr = self.transforms(image=y_arr)[\"image\"]\n",
    "            item[\"label\"] = y_arr\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing benchmark_src/cloud_model.py\n"
     ]
    }
   ],
   "source": [
    "%%file {submission_dir}/cloud_model.py\n",
    "from typing import Optional, List\n",
    "\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from cloud_dataset import CloudDataset\n",
    "    from losses import intersection_over_union\n",
    "except ImportError:\n",
    "    from benchmark_src.cloud_dataset import CloudDataset\n",
    "    from benchmark_src.losses import intersection_over_union\n",
    "\n",
    "\n",
    "class CloudModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bands: List[str],\n",
    "        x_train: Optional[pd.DataFrame] = None,\n",
    "        y_train: Optional[pd.DataFrame] = None,\n",
    "        x_val: Optional[pd.DataFrame] = None,\n",
    "        y_val: Optional[pd.DataFrame] = None,\n",
    "        hparams: dict = {},\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiate the CloudModel class based on the pl.LightningModule\n",
    "        (https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html).\n",
    "\n",
    "        Args:\n",
    "            bands (list[str]): Names of the bands provided for each chip\n",
    "            x_train (pd.DataFrame, optional): a dataframe of the training features with a row for each chip.\n",
    "                There must be a column for chip_id, and a column with the path to the TIF for each of bands.\n",
    "                Required for model training\n",
    "            y_train (pd.DataFrame, optional): a dataframe of the training labels with a for each chip\n",
    "                and columns for chip_id and the path to the label TIF with ground truth cloud cover.\n",
    "                Required for model training\n",
    "            x_val (pd.DataFrame, optional): a dataframe of the validation features with a row for each chip.\n",
    "                There must be a column for chip_id, and a column with the path to the TIF for each of bands.\n",
    "                Required for model training\n",
    "            y_val (pd.DataFrame, optional): a dataframe of the validation labels with a for each chip\n",
    "                and columns for chip_id and the path to the label TIF with ground truth cloud cover.\n",
    "                Required for model training\n",
    "            hparams (dict, optional): Dictionary of additional modeling parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hparams.update(hparams)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # required\n",
    "        self.bands = bands\n",
    "\n",
    "        # optional modeling params\n",
    "        self.backbone = self.hparams.get(\"backbone\", \"resnet34\")\n",
    "        self.weights = self.hparams.get(\"weights\", \"imagenet\")\n",
    "        self.learning_rate = self.hparams.get(\"lr\", 1e-3)\n",
    "        self.patience = self.hparams.get(\"patience\", 4)\n",
    "        self.num_workers = self.hparams.get(\"num_workers\", 2)\n",
    "        self.batch_size = self.hparams.get(\"batch_size\", 32)\n",
    "        self.gpu = self.hparams.get(\"gpu\", False)\n",
    "        self.transform = None\n",
    "\n",
    "        # Instantiate datasets, model, and trainer params if provided\n",
    "        self.train_dataset = CloudDataset(\n",
    "            x_paths=x_train,\n",
    "            bands=self.bands,\n",
    "            y_paths=y_train,\n",
    "            transforms=self.transform,\n",
    "        )\n",
    "        self.val_dataset = CloudDataset(\n",
    "            x_paths=x_val,\n",
    "            bands=self.bands,\n",
    "            y_paths=y_val,\n",
    "            transforms=None,\n",
    "        )\n",
    "        self.model = self._prepare_model()\n",
    "\n",
    "    ## Required LightningModule methods ##\n",
    "\n",
    "    def forward(self, image: torch.Tensor):\n",
    "        # Forward pass\n",
    "        return self.model(image)\n",
    "\n",
    "    def training_step(self, batch: dict, batch_idx: int):\n",
    "        \"\"\"\n",
    "        Training step.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): dictionary of items from CloudDataset of the form\n",
    "                {'chip_id': list[str], 'chip': list[torch.Tensor], 'label': list[torch.Tensor]}\n",
    "            batch_idx (int): batch number\n",
    "        \"\"\"\n",
    "        if self.train_dataset.data is None:\n",
    "            raise ValueError(\n",
    "                \"x_train and y_train must be specified when CloudModel is instantiated to run training\"\n",
    "            )\n",
    "\n",
    "        # Switch on training mode\n",
    "        self.model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        y = batch[\"label\"].long()\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "        # Forward pass\n",
    "        preds = self.forward(x)\n",
    "\n",
    "        # Log batch loss\n",
    "        loss = torch.nn.CrossEntropyLoss(reduction=\"none\")(preds, y).mean()\n",
    "        self.log(\n",
    "            \"loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: dict, batch_idx: int):\n",
    "        \"\"\"\n",
    "        Validation step.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): dictionary of items from CloudDataset of the form\n",
    "                {'chip_id': list[str], 'chip': list[torch.Tensor], 'label': list[torch.Tensor]}\n",
    "            batch_idx (int): batch number\n",
    "        \"\"\"\n",
    "        if self.val_dataset.data is None:\n",
    "            raise ValueError(\n",
    "                \"x_val and y_val must be specified when CloudModel is instantiated to run validation\"\n",
    "            )\n",
    "\n",
    "        # Switch on validation mode\n",
    "        self.model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        y = batch[\"label\"].long()\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "        # Forward pass & softmax\n",
    "        preds = self.forward(x)\n",
    "        preds = torch.softmax(preds, dim=1)[:, 1]\n",
    "        preds = (preds > 0.5) * 1  # convert to int\n",
    "\n",
    "        # Log batch IOU\n",
    "        batch_iou = intersection_over_union(preds, y)\n",
    "        self.log(\n",
    "            \"iou\", batch_iou, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return batch_iou\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # DataLoader class for training\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # DataLoader class for validation\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=0,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\n",
    "        return [opt], [sch]\n",
    "\n",
    "    ## Convenience Methods ##\n",
    "\n",
    "    def _prepare_model(self):\n",
    "        # Instantiate U-Net model\n",
    "        unet_model = smp.Unet(\n",
    "            encoder_name=self.backbone,\n",
    "            encoder_weights=self.weights,\n",
    "            in_channels=4,\n",
    "            classes=2,\n",
    "        )\n",
    "        if self.gpu:\n",
    "            unet_model.cuda()\n",
    "\n",
    "        return unet_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_paths(df, feature_dir, label_dir=None, bands=BANDS):\n",
    "    \"\"\"\n",
    "    Given dataframe with a column for chip_id, returns a dataframe with a column\n",
    "    added indicating the path to each band's TIF image as \"{band}_path\", eg \"B02_path\".\n",
    "    A column is also added to the dataframe with paths to the label TIF, if the\n",
    "    path to the labels directory is provided.\n",
    "    \"\"\"\n",
    "    for band in bands:\n",
    "        df[f\"{band}_path\"] = feature_dir / df[\"chip_id\"] / f\"{band}.tif\"\n",
    "        # make sure a random sample of paths exist\n",
    "        assert df.sample(n=40, random_state=5)[f\"{band}_path\"].path.exists().all()\n",
    "    if label_dir is not None:\n",
    "        df[\"label_path\"] = label_dir / (df[\"chip_id\"] + \".tif\")\n",
    "        # make sure a random sample of paths exist\n",
    "        assert df.sample(n=40, random_state=5)[\"label_path\"].path.exists().all()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing benchmark_src/losses.py\n"
     ]
    }
   ],
   "source": [
    "%%file {submission_dir}/losses.py\n",
    "import numpy as np\n",
    "\n",
    "# Loss function\n",
    "def intersection_over_union(pred, true):\n",
    "    \"\"\"\n",
    "    Calculates intersection and union for a batch of images.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): a tensor of predictions\n",
    "        true (torc.Tensor): a tensor of labels\n",
    "\n",
    "    Returns:\n",
    "        intersection (int): total intersection of pixels\n",
    "        union (int): total union of pixels\n",
    "    \"\"\"\n",
    "    valid_pixel_mask = true.ne(255)  # valid pixel mask\n",
    "    true = true.masked_select(valid_pixel_mask).to(\"cpu\")\n",
    "    pred = pred.masked_select(valid_pixel_mask).to(\"cpu\")\n",
    "\n",
    "    # Intersection and union totals\n",
    "    intersection = np.logical_and(true, pred)\n",
    "    union = np.logical_or(true, pred)\n",
    "    return intersection.sum() / union.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='split-data'></a>\n",
    "\n",
    "# Split the data\n",
    "\n",
    "To train our model, we want to separate the data into a \"training\" set and a \"validation\" set. That way we'll have a portion of labelled data that was not used in model training, which can give us a more accurate sense of how our model will perform on the competition test data. Remember, none of the test set locations are in competition training data, so your model's will performance will ultimately be measured on unseen locations.\n",
    "\n",
    "We have chosen the simplest route, and split our training chips randomly into 1/3 validation and 2/3 training. You may want to think about splitting by location instead of by chip, to better check how your model will do in new settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chip_id</th>\n",
       "      <th>location</th>\n",
       "      <th>datetime</th>\n",
       "      <th>cloudpath</th>\n",
       "      <th>B02_path</th>\n",
       "      <th>B03_path</th>\n",
       "      <th>B04_path</th>\n",
       "      <th>B08_path</th>\n",
       "      <th>label_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adwp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwp</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/adwp.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adwu</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwu</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwu/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwu/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwu/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwu/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/adwu.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adwz</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwz</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwz/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwz/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwz/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwz/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/adwz.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adxp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adxp</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adxp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adxp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adxp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adxp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/adxp.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aeaj</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/aeaj</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeaj/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeaj/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeaj/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeaj/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/aeaj.tif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chip_id  location              datetime                   cloudpath  \\\n",
       "0    adwp  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwp   \n",
       "1    adwu  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwu   \n",
       "2    adwz  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwz   \n",
       "3    adxp  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adxp   \n",
       "4    aeaj  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/aeaj   \n",
       "\n",
       "                                            B02_path  \\\n",
       "0  /driven-data/cloud-cover/train_features/adwp/B...   \n",
       "1  /driven-data/cloud-cover/train_features/adwu/B...   \n",
       "2  /driven-data/cloud-cover/train_features/adwz/B...   \n",
       "3  /driven-data/cloud-cover/train_features/adxp/B...   \n",
       "4  /driven-data/cloud-cover/train_features/aeaj/B...   \n",
       "\n",
       "                                            B03_path  \\\n",
       "0  /driven-data/cloud-cover/train_features/adwp/B...   \n",
       "1  /driven-data/cloud-cover/train_features/adwu/B...   \n",
       "2  /driven-data/cloud-cover/train_features/adwz/B...   \n",
       "3  /driven-data/cloud-cover/train_features/adxp/B...   \n",
       "4  /driven-data/cloud-cover/train_features/aeaj/B...   \n",
       "\n",
       "                                            B04_path  \\\n",
       "0  /driven-data/cloud-cover/train_features/adwp/B...   \n",
       "1  /driven-data/cloud-cover/train_features/adwu/B...   \n",
       "2  /driven-data/cloud-cover/train_features/adwz/B...   \n",
       "3  /driven-data/cloud-cover/train_features/adxp/B...   \n",
       "4  /driven-data/cloud-cover/train_features/aeaj/B...   \n",
       "\n",
       "                                            B08_path  \\\n",
       "0  /driven-data/cloud-cover/train_features/adwp/B...   \n",
       "1  /driven-data/cloud-cover/train_features/adwu/B...   \n",
       "2  /driven-data/cloud-cover/train_features/adwz/B...   \n",
       "3  /driven-data/cloud-cover/train_features/adxp/B...   \n",
       "4  /driven-data/cloud-cover/train_features/aeaj/B...   \n",
       "\n",
       "                                       label_path  \n",
       "0  /driven-data/cloud-cover/train_labels/adwp.tif  \n",
       "1  /driven-data/cloud-cover/train_labels/adwu.tif  \n",
       "2  /driven-data/cloud-cover/train_labels/adwz.tif  \n",
       "3  /driven-data/cloud-cover/train_labels/adxp.tif  \n",
       "4  /driven-data/cloud-cover/train_labels/aeaj.tif  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta = add_paths(train_meta, TRAIN_FEATURES, TRAIN_LABELS)\n",
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = train_val_test_split(train_meta, column='location', pct_train=0.6, pct_val=0.2, pct_test=0.2, random_state=42)                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7017, 9), (2349, 9), (2382, 9))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features from labels\n",
    "feature_cols = [\"chip_id\"] + [f\"{band}_path\" for band in BANDS]\n",
    "\n",
    "train_x = train[feature_cols].copy()\n",
    "train_y = train[[\"chip_id\", \"label_path\"]].copy()\n",
    "\n",
    "val_x = val[feature_cols].copy()\n",
    "val_y = val[[\"chip_id\", \"label_path\"]].copy()\n",
    "\n",
    "test_x = test[feature_cols].copy()\n",
    "test_y = test[[\"chip_id\", \"label_path\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create benchmark_src folder\n",
    "# submission_dir = Path(\"benchmark_src\")\n",
    "# if submission_dir.exists():\n",
    "#     shutil.rmtree(submission_dir)\n",
    "\n",
    "# submission_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run inference in this notebook by importing classes and functions from the scripts in `benchmark_src`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.8/site-packages/pretrainedmodels/models/dpn.py:255: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if block_type is 'proj':\n",
      "/srv/conda/envs/notebook/lib/python3.8/site-packages/pretrainedmodels/models/dpn.py:258: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif block_type is 'down':\n",
      "/srv/conda/envs/notebook/lib/python3.8/site-packages/pretrainedmodels/models/dpn.py:262: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  assert block_type is 'normal'\n"
     ]
    }
   ],
   "source": [
    "from benchmark_src.cloud_model import CloudModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fit-model'></a>\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "It's go time! The only required arguments to fit the model are the train features, the train labels, the validation features, and the validation labels. \n",
    "\n",
    "We've opted to train with all of the defaults hyperparameters defined in `__init__` for simplicity, so there is a lot of room for tweaks and improvements! You can experiment with things like learning rate, patience, batch size, and much much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "# Set up pytorch_lightning.Trainer object\n",
    "cloud_model = CloudModel(\n",
    "    bands=BANDS,\n",
    "    x_train=train_x,\n",
    "    y_train=train_y,\n",
    "    x_val=val_x,\n",
    "    y_val=val_y,\n",
    "    hparams={\"num_workers\": 7, \"batch_size\": 8},\n",
    ")\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"iou_epoch\", mode=\"max\", verbose=True\n",
    ")\n",
    "early_stopping_callback = pl.callbacks.early_stopping.EarlyStopping(\n",
    "    monitor=\"iou_epoch\",\n",
    "    patience=(cloud_model.patience * 3),\n",
    "    mode=\"max\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    fast_dev_run=False,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "trainer.fit(model=cloud_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='generate-submission'></a>\n",
    "\n",
    "# Generate a submission\n",
    "\n",
    "Now that we have our trained model, we can generate a full submission. **Remember that this is a [code execution](https://www.drivendata.org/competitions/83/cloud-cover/page/412/) competition,** so you will be submitting our inference code rather than our predictions. We've already written out our key class definition to scripts in the folder `benchmark_src`, which now contains:\n",
    "\n",
    "```\n",
    "benchmark_src\n",
    "├── cloud_dataset.py\n",
    "├── cloud_model.py\n",
    "└── losses.py\n",
    "```\n",
    "\n",
    "To submit to the competition, we still need to:\n",
    "\n",
    "1. Store our trained model weights in `benchmark_src` so that they can be loaded during inference\n",
    "\n",
    "2. Write a `main.py` file that loads our model weights, generates predictions for each chip, and saves the predictions to a folder called `predictions` in the same directory as itself\n",
    "\n",
    "3. Zip the contents of `benchmark_src/` - not the directory itself - into a file called `submission.zip`. \n",
    "\n",
    "4. Upload `submission.zip` to the competition submissions page. The file will be unzipped and `main.py` will be run in a [containerized execution environment](https://github.com/drivendataorg/cloud-cover-runtime) to calculate our model's IOU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Save our model\n",
    "\n",
    "First, let's make a folder for our model assets, and save the weights for our trained model using PyTorch's handy `model.save()` method. The below saves the weights to `benchmark_src/assets/cloud_model.pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "submission_assets_dir = submission_dir / \"assets\"\n",
    "submission_assets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_weight_path = submission_assets_dir / \"cloud_model.pt\"\n",
    "torch.save(cloud_model.state_dict(), model_weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Write `main.py`\n",
    "\n",
    "Now we'll write out a script called `main.py` to `benchmark_src`, which runs the whole inference process using the saved model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing benchmark_src/main.py\n"
     ]
    }
   ],
   "source": [
    "%%file benchmark_src/main.py\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import typer\n",
    "\n",
    "try:\n",
    "    from cloud_dataset import CloudDataset\n",
    "    from cloud_model import CloudModel\n",
    "except ImportError:\n",
    "    from benchmark_src.cloud_dataset import CloudDataset\n",
    "    from benchmark_src.cloud_model import CloudModel\n",
    "\n",
    "\n",
    "ROOT_DIRECTORY = Path(\"/codeexecution\")\n",
    "PREDICTIONS_DIRECTORY = ROOT_DIRECTORY / \"predictions\"\n",
    "ASSETS_DIRECTORY = ROOT_DIRECTORY / \"assets\"\n",
    "DATA_DIRECTORY = ROOT_DIRECTORY / \"data\"\n",
    "INPUT_IMAGES_DIRECTORY = DATA_DIRECTORY / \"test_features\"\n",
    "\n",
    "# Set the pytorch cache directory and include cached models in your submission.zip\n",
    "os.environ[\"TORCH_HOME\"] = str(ASSETS_DIRECTORY / \"assets/torch\")\n",
    "\n",
    "\n",
    "def get_metadata(features_dir: os.PathLike, bands: List[str]):\n",
    "    \"\"\"\n",
    "    Given a folder of feature data, return a dataframe where the index is the chip id\n",
    "    and there is a column for the path to each band's TIF image.\n",
    "\n",
    "    Args:\n",
    "        features_dir (os.PathLike): path to the directory of feature data, which should have\n",
    "            a folder for each chip\n",
    "        bands (list[str]): list of bands provided for each chip\n",
    "    \"\"\"\n",
    "    chip_metadata = pd.DataFrame(index=[f\"{band}_path\" for band in bands])\n",
    "    chip_ids = (\n",
    "        pth.name for pth in features_dir.iterdir() if not pth.name.startswith(\".\")\n",
    "    )\n",
    "\n",
    "    for chip_id in chip_ids:\n",
    "        chip_bands = [features_dir / chip_id / f\"{band}.tif\" for band in bands]\n",
    "        chip_metadata[chip_id] = chip_bands\n",
    "\n",
    "    return chip_metadata.transpose().reset_index().rename(columns={\"index\": \"chip_id\"})\n",
    "\n",
    "\n",
    "def make_predictions(\n",
    "    model: CloudModel,\n",
    "    x_paths: pd.DataFrame,\n",
    "    bands: List[str],\n",
    "    predictions_dir: os.PathLike,\n",
    "):\n",
    "    \"\"\"Predicts cloud cover and saves results to the predictions directory.\n",
    "\n",
    "    Args:\n",
    "        model (CloudModel): an instantiated CloudModel based on pl.LightningModule\n",
    "        x_paths (pd.DataFrame): a dataframe with a row for each chip. There must be a column for chip_id,\n",
    "                and a column with the path to the TIF for each of bands provided\n",
    "        bands (list[str]): list of bands provided for each chip\n",
    "        predictions_dir (os.PathLike): Destination directory to save the predicted TIF masks\n",
    "    \"\"\"\n",
    "    test_dataset = CloudDataset(x_paths=x_paths, bands=bands)\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=model.batch_size,\n",
    "        num_workers=model.num_workers,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    for batch_index, batch in enumerate(test_dataloader):\n",
    "        logger.debug(f\"Predicting batch {batch_index} of {len(test_dataloader)}\")\n",
    "        x = batch[\"chip\"]\n",
    "        preds = model.forward(x)\n",
    "        preds = torch.softmax(preds, dim=1)[:, 1]\n",
    "        preds = (preds > 0.5).detach().numpy().astype(\"uint8\")\n",
    "        for chip_id, pred in zip(batch[\"chip_id\"], preds):\n",
    "            chip_pred_path = predictions_dir / f\"{chip_id}.tif\"\n",
    "            chip_pred_im = Image.fromarray(pred)\n",
    "            chip_pred_im.save(chip_pred_path)\n",
    "\n",
    "\n",
    "def main(\n",
    "    model_weights_path: Path = ASSETS_DIRECTORY / \"cloud_model.pt\",\n",
    "    test_features_dir: Path = DATA_DIRECTORY / \"test_features\",\n",
    "    predictions_dir: Path = PREDICTIONS_DIRECTORY,\n",
    "    bands: List[str] = [\"B02\", \"B03\", \"B04\", \"B08\"],\n",
    "    fast_dev_run: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate predictions for the chips in test_features_dir using the model saved at\n",
    "    model_weights_path.\n",
    "\n",
    "    Predictions are saved in predictions_dir. The default paths to all three files are based on\n",
    "    the structure of the code execution runtime.\n",
    "\n",
    "    Args:\n",
    "        model_weights_path (os.PathLike): Path to the weights of a trained CloudModel.\n",
    "        test_features_dir (os.PathLike, optional): Path to the features for the test data. Defaults\n",
    "            to 'data/test_features' in the same directory as main.py\n",
    "        predictions_dir (os.PathLike, optional): Destination directory to save the predicted TIF masks\n",
    "            Defaults to 'predictions' in the same directory as main.py\n",
    "        bands (List[str], optional): List of bands provided for each chip\n",
    "    \"\"\"\n",
    "    if not test_features_dir.exists():\n",
    "        raise ValueError(\n",
    "            f\"The directory for test feature images must exist and {test_features_dir} does not exist\"\n",
    "        )\n",
    "    predictions_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    logger.info(\"Loading model\")\n",
    "    model = CloudModel(bands=bands, hparams={\"weights\": None})\n",
    "    model.load_state_dict(torch.load(model_weights_path))\n",
    "\n",
    "    logger.info(\"Loading test metadata\")\n",
    "    test_metadata = get_metadata(test_features_dir, bands=bands)\n",
    "    if fast_dev_run:\n",
    "        test_metadata = test_metadata.head()\n",
    "    logger.info(f\"Found {len(test_metadata)} chips\")\n",
    "\n",
    "    logger.info(\"Generating predictions in batches\")\n",
    "    make_predictions(model, test_metadata, bands, predictions_dir)\n",
    "\n",
    "    logger.info(f\"\"\"Saved {len(list(predictions_dir.glob(\"*.tif\")))} predictions\"\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    typer.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to test out running `main` from this notebook, we could execute:\n",
    "\n",
    "```python\n",
    "from benchmark_src.main import main\n",
    "\n",
    "main(\n",
    "    model_weights_path=submission_dir / \"assets/cloud_model.pt\",\n",
    "    test_features_dir=TRAIN_FEATURES,\n",
    "    predictions_dir=submission_dir / \"predictions\",\n",
    "    fast_dev_run=True,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Zip submission contents\n",
    "\n",
    "Compress all of the submission files in `benchmark_src` into a .zip called `submission.zip`. Our final submission directory has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear out our pycache before zipping up submission\n",
    "!rm -rf benchmark_src/__pycache__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mbenchmark_src\u001b[00m\n",
      "├── \u001b[01;34massets\u001b[00m\n",
      "│   └── cloud_model.pt\n",
      "├── cloud_dataset.py\n",
      "├── cloud_model.py\n",
      "├── losses.py\n",
      "└── main.py\n",
      "\n",
      "1 directory, 5 files\n"
     ]
    }
   ],
   "source": [
    "!tree benchmark_src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to make sure that your submission does *not* include any prediction files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: assets/ (stored 0%)\n",
      "updating: assets/cloud_model.pt (deflated 7%)\n",
      "updating: cloud_dataset.py (deflated 63%)\n",
      "updating: cloud_model.py (deflated 74%)\n",
      "updating: losses.py (deflated 57%)\n",
      "updating: main.py (deflated 64%)\n"
     ]
    }
   ],
   "source": [
    "# Zip submission\n",
    "!cd benchmark_src && zip -r ../submission.zip *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87M\tsubmission.zip\n"
     ]
    }
   ],
   "source": [
    "!du -h submission.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload submission"
   ]
  },
  {
   "attachments": {
    "4bc76ab5-aa2c-4eab-971d-e398722f30bd.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now head over to the competition [submissions page](https://www.drivendata.org/competitions/83/cloud-cover/submissions/) to upload our code and get our model's IOU!\n",
    "\n",
    "![image.png](attachment:4bc76ab5-aa2c-4eab-971d-e398722f30bd.png)\n",
    "\n",
    "Our submission took about 20 minutes to execute. You can monitor progress during scoring with the Code Execution Status [tab](https://www.drivendata.org/competitions/83/submissions/code/). Finally, we see that we got an IOU of **0.817** - that's pretty good! It means that 81.7% of the area covered by either the ground truth labels or our predictions was shared between the two.\n",
    "\n",
    "There is still plenty of room for improvement! Head over to the On Cloud N challenge [homepage](https://www.drivendata.org/competitions/83/cloud-cover/page/396/) to get started on your own model. We're excited to see what you create!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
